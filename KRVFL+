# -*- coding: utf-8 -*-
"""DENORMALIZED_KRVFL+.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1WD5nle0fcRxKFWuBN2wbcE1US___zjE7

# Extracting data from url
"""

import yfinance as yf
from datetime import datetime

def get_daily_stock_data(symbol, start_date, end_date):
    try:
        stock_data = yf.download(symbol, start=start_date, end=end_date)
        return stock_data
    except Exception as e:
        print(f"An error occurred: {e}")
        return None

# Example usage:
symbol = 'TATAMOTORS.NS'

# Set the start and end dates
start_date = '2018-11-10'
end_date = '2023-12-10'

daily_data = get_daily_stock_data(symbol, start_date, end_date)

if daily_data is not None and not daily_data.empty:
    print(f"Daily Stock Data for {symbol} (from {start_date} to {end_date}):")
    print(daily_data.head())
else:
    print(f"Failed to retrieve daily stock data for {symbol}.")

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

data = daily_data

"""# data preparation"""

# Function to create sliding windows for the 'Close' column
def create_sliding_windows(data_column, window_size):
    windows = []
    for i in range(len(data_column) - window_size + 1):
        window = data_column.iloc[i:i + window_size]
        windows.append(window.values.flatten())
    return pd.DataFrame(windows)

# Set your desired window size
window_size = 5

# Create sliding windows for the 'Close' column
X_input = create_sliding_windows(data['Close'], window_size)

# Display the resulting 2D matrix
print(X_input[:5])

"""# rowise normalization"""

import pandas as pd
import numpy as np

# Assuming 'data' is a 1231x5 DataFrame
# Set your desired window size
window_size = 5

# Lists to store X_min and X_max for each iteration
X_min_list = []
X_max_list = []

# Function to normalize each row independently to the range [0, 1]
def row_wise_normalization(X_input):
    normalized_windows = []

    for i in range(len(X_input)):
        # Extract features (X) and target variable (Y)
        X_window = X_input.iloc[i, :-1]
        Y_window = X_input.iloc[i, -1]

        # Store X_min and X_max
        X_min = X_window.min()
        X_max = X_window.max()
        X_min_list.append(X_min)
        X_max_list.append(X_max)

        # Normalize each element in the row of X to the range [0, 1]
        X_normalized = (X_window - X_min) / (X_max - X_min)

        # Normalize the last element in the row of Y to the range [0, 1]
        Y_normalized = (Y_window - X_min) / (X_max - X_min)

        # Append the normalized X and Y to the list
        normalized_row = np.concatenate([X_normalized.values, np.array([Y_normalized])])
        normalized_windows.append(normalized_row)

    # Create arrays from the lists
    X_min_array = np.array(X_min_list)
    X_max_array = np.array(X_max_list)

    # Create a matrix from the list of normalized rows
    normalized_data = np.array(normalized_windows)

    return normalized_data, X_min_array, X_max_array

# Apply row-wise normalization
normalized_windows, X_min_array, X_max_array = row_wise_normalization(X_input)

# Display the resulting 2D matrix with normalized values
print("Normalized Data:")
print(normalized_windows[:5])
print(normalized_windows.shape)

"""# data preparation X_previlege"""

# Function to create sliding windows for the 'Volume' column
def create_sliding_windows(data_column, window_size):
    windows = []
    for i in range(len(data_column) - window_size + 1):
        window = data_column.iloc[i:i + window_size]
        windows.append(window.values.flatten())
    return pd.DataFrame(windows)

# Set your desired window size
window_size = 5

# Create sliding windows for the 'Close' column
previledged = create_sliding_windows(data['Volume'], window_size)

# Display the resulting 2D matrix
print(previledged[:5])

"""# rowise normalization"""

import pandas as pd
import numpy as np

# Assuming 'data' is a 1231x5 DataFrame
# Set your desired window size
window_size = 5

# Function to normalize each row independently to the range [0, 1]
def row_wise_normalization(previledged):
    normalized_windows = []

    for i in range(len(previledged)):
        # Extract features (X) and target variable (Y)
        X_window = previledged.iloc[i, :-1]
        Y_window = previledged.iloc[i, -1]

        # Normalize each element in the row of X to the range [0, 1]
        X_normalized = (X_window - X_window.min()) / (X_window.max() - X_window.min())

        # Normalize the last element in the row of Y to the range [0, 1]
        Y_normalized = (Y_window - X_window.min()) / (X_window.max() - X_window.min())

        # Append the normalized X and Y to the list
        normalized_row = np.concatenate([X_normalized.values, np.array([Y_normalized])])
        normalized_windows.append(normalized_row)

    # Create a matrix from the list of normalized rows
    normalized_data = np.array(normalized_windows)

    return normalized_data

# Apply row-wise normalization
X_new = row_wise_normalization(previledged)

# Display the resulting 2D matrix with normalized values
print("Normalized Data:")
print(X_new[:5])
print(X_new.shape)

from sklearn.model_selection import train_test_split

"""# splitting the data into test and train"""

# Split the dataset into training and testing sets
X_priviledged, X_privileged_test = train_test_split(X_new,test_size=0.2, random_state=42)

X_privileged = X_priviledged[:,:-1]
X_privileged.shape
X_privileged_test[:,-1]

X = normalized_windows[:, :-1]
y = normalized_windows[:, -1]

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)

# Split X_min and X_max into training and testing sets
X_min_train, X_min_test, X_max_train, X_max_test = train_test_split(X_min_array, X_max_array, test_size=0.2, random_state=42)

"""# Cross validation"""

import numpy as np
from sklearn.metrics import mean_squared_error

def sigmoid(weighted_sum_fold):
    return 1 / (1 + np.exp(-weighted_sum_fold))

def perform_cross_validation(X_train, y_train):
    num_folds = 10
    fold_size = len(X_train) // num_folds
    A_values = np.logspace(-5, 5, 11)
    best_mse = float('inf')
    best_A = None
    hidden_weights_fold = np.random.rand(X_train.shape[1], 50)

    for A in A_values:
        total_mse = 0.0

        for fold in range(num_folds):
            start_idx = fold * fold_size
            end_idx = (fold + 1) * fold_size
            X_valid_fold = X_train[start_idx:end_idx]
            y_valid_fold = y_train[start_idx:end_idx]

            X_train_fold = np.concatenate([X_train[:start_idx], X_train[end_idx:]])
            y_train_fold = np.concatenate([y_train[:start_idx], y_train[end_idx:]])

            weighted_sum_fold = np.dot(X_train_fold, hidden_weights_fold)
            activation_result_fold = sigmoid(weighted_sum_fold)
            augmented_X_train_fold = np.hstack((X_train_fold, activation_result_fold))

            D_transpose_fold = np.transpose(augmented_X_train_fold)
            term_inside_parentheses_fold = np.dot(augmented_X_train_fold, D_transpose_fold) + A * np.identity(X_train_fold.shape[0])
            inverse_term_fold = np.linalg.inv(term_inside_parentheses_fold)
            Half_fold = np.dot(D_transpose_fold, inverse_term_fold)
            output_weights_fold = np.dot(Half_fold, y_train_fold)

            hidden_weights_valid_fold = np.dot(X_valid_fold, hidden_weights_fold)
            activation_result_valid_fold = sigmoid(hidden_weights_valid_fold)
            augmented_X_valid_fold = np.hstack((X_valid_fold, activation_result_valid_fold))
            predicted_output_fold = np.dot(augmented_X_valid_fold, output_weights_fold)

            mse_fold = mean_squared_error(y_valid_fold, predicted_output_fold)
            total_mse += mse_fold

        average_mse = total_mse / num_folds

        print(f"For A={A}, Average MSE: {average_mse}")

        if average_mse < best_mse:
            best_mse = average_mse
            best_A = A

    print(f"Best Average MSE: {best_mse} achieved with A={best_A}")
    return best_A

best_A = perform_cross_validation(X_train, y_train)

"""# Kernal functions"""

from sklearn.metrics.pairwise import rbf_kernel, linear_kernel
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score


# Assuming you have kernel functions like these, replace them with your actual kernel functions
def linear_kernel(X, Y):
    return np.dot(X, Y.T)

def gaussian_kernel(X, Y, gamma=1.0):
    return np.exp(-gamma * np.linalg.norm(X[:, np.newaxis] - Y, axis=2) ** 2)

"""# function calling"""

gamma =0.1
# Define kernel matrices
Ω1 = linear_kernel(X_train, X_train)
Ω2 = gaussian_kernel(X_train, X_train,gamma)
Ω1_tilde = linear_kernel(X_privileged, X_privileged)
Ω2_tilde = gaussian_kernel(X_privileged, X_privileged,gamma)

print(Ω1.shape,
Ω2.shape,
Ω1_tilde.shape,
Ω2_tilde.shape )

"""# Normal Rvfl weight calculation without using Activation function

#using kernal function

#first training and testing the data using kernal function
"""

# Applying sigmoid to the total
result = Ω1
# Augmenting the input features
augmented_X_train = np.hstack((X_train, result))
D = augmented_X_train
D_transpose = np.transpose(D)
A = best_A  # Regularization parameter
term_inside_parentheses = np.dot(D, D_transpose) + A * np.identity(X_train.shape[0])
inverse_term = np.linalg.inv(term_inside_parentheses)
Half = np.dot(D_transpose, inverse_term)
# Calculate the output weights using the formula
output_weights = np.dot(Half, y_train)
print("Output weights")
print(output_weights[:5])
print(output_weights.shape)

# ... (previous code)
activation_result_test = gaussian_kernel(X_test,X_train)
augmented_X_test = np.hstack((X_test, activation_result_test))
predicted_output_test = np.dot(augmented_X_test, output_weights)

"""#predicted output generated by Normal RVFL using kernal functions"""

predicted_output_test[:5]

"""# krvfl+ base formula"""

# Calculate t
A = best_A
C = 0.1
t = Ω1 + Ω2 + 1/C * (Ω1_tilde + Ω2_tilde) + np.identity(len(X_privileged)) / A
l=(Ω1_tilde + Ω2_tilde)

# Convert y_train to a NumPy array and reshape if needed
#y_train_array = y_train.to_numpy().reshape((-1, 1))

# Calculate k as a column vector
ee = np.ones(Ω1_tilde.shape[1])
k = y_train -(1/C)*np.dot(l,(ee.T))

# Calculate the inverse of t
t_inverse = np.linalg.inv(t)

# Calculate W
w = np.dot(t_inverse, k)

"""#predicted output using krvfl+ fromulation"""

# Assuming Ω1_test and Ω2_test are calculated similarly to Ω1 and Ω2
Ω1_test = linear_kernel(X_test, X_train)
Ω2_test = gaussian_kernel(X_test, X_train, gamma)

# Calculate predicted output using the trained weights (W)
predicted_output = np.dot(Ω1_test + Ω2_test,w)

"""# Augementaion of the pridicted outputs"""

predicted_output_p = sigmoid(predicted_output) + sigmoid(predicted_output_test)

y_test[:5]

"""# MSE and Graph representation"""

from sklearn.metrics import mean_squared_error
import seaborn as sns
import matplotlib.pyplot as plt

# Calculate Mean Squared Error
mse = mean_squared_error(y_test, predicted_output_p)
print(f"Mean Squared Error: {mse}")

# Plotting the results
plt.figure(figsize=(10, 6))
sns.lineplot(x=range(len(y_test)), y=y_test, label="Actual Values")
sns.lineplot(x=range(len(predicted_output_p)), y=predicted_output_p, label="Predicted Values")

# Set labels and legend
plt.xlabel('Sample Index')
plt.ylabel('Target Value')
plt.legend()
plt.title('Actual vs Predicted Values with MSE')

# Show the plot
plt.show()

# Assuming predicted_output is the array containing predicted values
denormalized_output = []

for i in range(len(X_min_test)):
    # Extract the normalized predicted output
    normalized_prediction = predicted_output_p[i]

    # Extract the corresponding X_min and X_max values
    X_min = X_min_test[i]
    X_max = X_max_test[i]

    # Denormalize the predicted output
    denormalized_prediction = normalized_prediction * (X_max - X_min) + X_min

    # Append the denormalized prediction to the list
    denormalized_output.append(denormalized_prediction)

# Display the denormalized output
print("Denormalized Output:")
print(denormalized_output[:5])
